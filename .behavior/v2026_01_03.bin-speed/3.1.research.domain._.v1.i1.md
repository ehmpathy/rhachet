# domain research: bin-speed optimization

## .context

the wish is to ensure rhachet CLI invocations add no more than 100ms latency on top of direct script execution. current measurements show:

- **direct shell script**: ~174ms avg (p95: 201ms) [1]
- **rhachet CLI invocation**: 5.6-6.4s avg (p95: 6.4s) [1]
- **overhead factor**: ~35x slower [1]

this overhead makes rhachet unsuitable for latency-sensitive operations like Claude Code PreToolUse hooks (5s timeout) [1].

---

## domain objects

### entities

| entity | description | unique key | primary key |
|--------|-------------|------------|-------------|
| `RoleRegistry` | collection of roles for a repo/context | `{ repo, role }` | internal |
| `Role` | named actor with skills/briefs/inits | `{ name }` | internal |
| `RoleSkill` | executable skill within a role | `{ name, role }` | internal |
| `RoleInitExecutable` | initialization script for a role | `{ name, role }` | internal |
| `Actor` | instantiated role with context | runtime only | - |

### events

| event | description | trigger |
|-------|-------------|---------|
| `CliInvoke` | CLI command execution started | `npx rhachet ...` |
| `SkillRun` | skill execution started | `rhachet run --skill X` |
| `RoleBoot` | role context loaded | `rhachet roles boot` |

### literals

| literal | description |
|---------|-------------|
| `InvokeOpts` | CLI invocation options (config, role, etc) |
| `RoleContext` | loaded context for a role (briefs, skills, stats) |

---

## domain operations

### current invocation chain

```
bin/run                          # shebang: node --experimental-strip-types
  └─> dist/contract/cli/invoke   # precompiled JS
       └─> commander.parse()     # CLI framework
            └─> getRegistriesByOpts()    # loads rhachet.use.ts config
                 └─> require(rhachet.use.ts)
                      └─> getRoleRegistries()
                           └─> multiple RoleRegistry instances
            └─> getBrainReplsByOpts()    # optional brain setup
            └─> getInvokeHooksByOpts()   # optional hooks
            └─> invokeRun/invokeRoles... # actual command
```

### critical operations for performance

| operation | type | impact |
|-----------|------|--------|
| `getRegistriesByOpts` | getOne/getAll | **high** - loads all registries on every invocation |
| `getGitRepoRoot` | getOne | medium - filesystem traversal |
| `assureUniqueRoles` | validation | low |
| `program.parseAsync` | parse | low - commander is lightweight |

---

## relationships

### treestruct of decoration

```
RoleRegistry
  └─> Role[]
       ├─> RoleSkill[]
       ├─> RoleBrief[]
       └─> RoleInitExecutable[]
```

### dependency graph

```
invoke.ts (entrypoint)
  ├─> commander (14.0.0)
  ├─> rhachet-artifact-git (getGitRepoRoot)
  ├─> domain.operations/invoke/*
  │    ├─> getRegistriesByOpts
  │    ├─> getBrainReplsByOpts
  │    └─> getInvokeHooksByOpts
  └─> invokeXxx subcommands
```

---

## root cause analysis

### where time is spent [2-10]

1. **node.js cold start**: ~60ms baseline [2]
2. **module resolution overhead**: hundreds of `require()` calls [3,4]
3. **typescript transpilation** (if using ts-node/tsx): adds 500ms+ [5,6]
4. **config loading**: dynamic import of `rhachet.use.ts`
5. **registry enumeration**: loading all role metadata upfront

### node.js `require()` hunting problem [3]

> "when you `require("express")` in your application, Node's require will try to load `node_modules/express.js` and fail, then try `express.json`, then `express.node`, before finally reading `package.json` to find the main filename. each wasted file system call takes only 100 microseconds, but the tiny delays add up to hundreds of milliseconds and finally seconds for larger frameworks." [3]

### key insight from esbuild [7]

> "a command-line application is a worst-case performance situation for a JIT-compiled language. every time you run your bundler, the JavaScript VM is seeing your bundler's code for the first time without any optimization hints. while esbuild is busy parsing your JavaScript, node is busy parsing your bundler's JavaScript. by the time node has finished parsing your bundler's code, esbuild might have already exited." [7]

---

## solution strategies

### strategy 1: precompile + bundle (esbuild)

bundle all dependencies into a single file, eliminating module resolution overhead.

| aspect | value |
|--------|-------|
| startup reduction | 10-100x faster than unbundled [7] |
| implementation | `esbuild --bundle --platform=node` |
| trade-off | larger single file, harder to debug |
| precedent | esbuild itself uses this approach [7] |

**benchmark reference**: "codebases can take less than 100ms to build and bundle up" [7]

### strategy 2: bun compile (single executable)

compile to native binary with embedded runtime.

| aspect | value |
|--------|-------|
| startup time | 38-52ms for CLI tools [8] |
| vs bun run | 2x faster than `bun run` [8] |
| vs node | ~4x faster cold starts [9] |
| binary size | ~56.8MB per platform [10] |
| implementation | `bun build --compile --bytecode` |

**benchmark reference**: "compiled executables reduce memory usage and improve Bun's start time. Normally, Bun reads and transpiles JavaScript and TypeScript files on import/require, which costs time and memory." [8]

### strategy 3: node.js SEA (single executable application)

native node.js approach using V8 code cache.

| aspect | value |
|--------|-------|
| status | stability 1.1 - active development [11] |
| startup gain | code cache skips compilation [11] |
| limitation | CommonJS only, no dynamic `import()` [11] |
| implementation | complex blob injection process |

**quote**: "instead of compiling the main script from scratch, Node.js would use the code cache to speed up the compilation" [11]

### strategy 4: lazy loading

defer loading of heavy dependencies until actually needed.

| aspect | value |
|--------|-------|
| reduction | 20-38% faster startup for individual modules [3] |
| implementation | dynamic imports inside command handlers |
| trade-off | first-use latency instead of startup latency |

**example**: only load `@anthropic-ai/sdk` when `rhachet ask` is invoked, not on every CLI call.

### strategy 5: tsx over ts-node

if runtime transpilation is needed, tsx is faster.

| aspect | value |
|--------|-------|
| startup | 5-10x faster than ts-node [5] |
| reason | uses esbuild backend [5] |
| trade-off | no type checking at runtime [5] |

**quote**: "tsx starts TypeScript files 5-10x faster than ts-node, making development iterations much more responsive" [5]

---

## composition to support wish

### recommended approach: hybrid (esbuild + lazy loading)

1. **bundle core CLI** with esbuild into single `dist/cli.bundle.js`
   - includes: commander, core invoke logic, subcommand routing
   - excludes: heavy deps (anthropic sdk, openai sdk, brain modules)

2. **lazy load on demand**
   - brain modules only when `ask`/`act` commands used
   - full registry loading only when needed

3. **optimize config loading**
   - cache compiled `rhachet.use.ts`
   - only reload when file changes (via mtime check)

### expected performance

| component | current | target |
|-----------|---------|--------|
| cold start | 5.6-6.4s | <100ms |
| `rhachet run` | ~6s | <200ms |
| `rhachet roles init` | ~6s | <300ms |
| `rhachet roles boot` | ~6s | <400ms (includes reading all briefs) |

### measurement strategy

to diagnose exactly where time is spent:

```ts
// add timing markers throughout invoke.ts
const t0 = performance.now();
// ... operation ...
console.debug(`[perf] operation took ${performance.now() - t0}ms`);
```

or use node's built-in profiling [12]:
```sh
node --cpu-prof ./bin/run roles boot
```

---

## citations

| # | source | url |
|---|--------|-----|
| 1 | PR #129 rhachet-roles-ehmpathy | `gh pr diff 129 --repo ehmpathy/rhachet-roles-ehmpathy` |
| 2 | Node.js Startup Series | https://www.kvakil.me/posts/2023-05-09-nodejs-startup-series-intro-and-measuring-startup-time.html |
| 3 | Faster Node app require | https://glebbahmutov.com/blog/faster-node-app-require/ |
| 4 | Ways to Improve Node.js Loader Performance | https://blog.appsignal.com/2025/10/22/ways-to-improve-nodejs-loader-performance.html |
| 5 | TSX vs ts-node Comparison | https://betterstack.com/community/guides/scaling-nodejs/tsx-vs-ts-node/ |
| 6 | Running TypeScript in Node.js | https://blog.logrocket.com/running-typescript-node-js-tsx-vs-ts-node-vs-native/ |
| 7 | esbuild FAQ | https://esbuild.github.io/faq/ |
| 8 | Bun Single-file executable docs | https://bun.com/docs/bundler/executables |
| 9 | Bun vs Node.js 2025 | https://strapi.io/blog/bun-vs-nodejs-performance-comparison-guide |
| 10 | Creating NPX compatible CLI tools with Bun | https://runspired.com/2025/01/25/npx-executables-with-bun.html |
| 11 | Node.js Single Executable Applications | https://nodejs.org/api/single-executable-applications.html |
| 12 | Node.js Profiling Applications | https://nodejs.org/en/learn/getting-started/profiling |

### key quotes

**on why node.js CLIs are slow [7]:**
> "a command-line application is a worst-case performance situation for a JIT-compiled language. every time you run your bundler, the JavaScript VM is seeing your bundler's code for the first time without any optimization hints."

**on module resolution overhead [3]:**
> "each wasted file system call takes only 100 microseconds, but the tiny delays add up to hundreds of milliseconds and finally seconds for larger frameworks."

**on bun compile benefits [8]:**
> "compiled executables reduce memory usage and improve Bun's start time. Normally, Bun reads and transpiles JavaScript and TypeScript files on import/require, which costs time and memory. With compiled executables, you can move that cost from runtime to build-time."

**on tsx performance [5]:**
> "tsx starts TypeScript files 5-10x faster than ts-node, making development iterations much more responsive... because it's using esbuild under the hood. Esbuild is written in Go, whereas most JavaScript build tools are written in JavaScript."

**on esbuild benchmark [7]:**
> "with esbuild, codebases can take less than 100ms to build and bundle up."

---

## next steps

1. **profile current invocation** - add timing markers to identify exact bottlenecks
2. **prototype esbuild bundle** - measure startup time improvement
3. **prototype bun compile** - compare binary approach
4. **implement acceptance tests** - codify 100ms overhead requirement
5. **implement chosen solution** - based on profiling + prototyping results

---

## appendix A: binary compilation deep dive

### why binary compilation is the maximal solution

binary compilation (bun compile) would give the fastest possible startup:

| approach | expected startup | vs current 6s |
|----------|------------------|---------------|
| bun --compile --bytecode | ~38ms | **158x faster** |
| bun --compile | ~52ms | **115x faster** |
| esbuild bundle (still node) | ~80-100ms | **60x faster** |

### detriments analysis

#### 1. binary size (~56MB per platform)

```
rhachet-linux-x64    56.8 MB
rhachet-linux-arm64  56.8 MB
rhachet-darwin-x64   56.8 MB
rhachet-darwin-arm64 56.8 MB
rhachet-win-x64.exe  56.8 MB
```

vs current npm package: ~2MB (relies on system node)

#### 2. cross-platform distribution complexity

currently: `npm install rhachet` works everywhere

with binary: need to either
- publish 5 separate binaries to npm (awkward)
- use postinstall script to download correct binary (like esbuild does)
- publish to homebrew/apt/etc separately

#### 3. dynamic config loading

rhachet loads `rhachet.use.ts` from the user's project at runtime:

```ts
const configPath = resolve(await getGitRepoRoot(), 'rhachet.use.ts');
const registries = await getRegistriesByOpts({ opts: { config: configPath } });
```

**with bun compile**: this still works — bun can dynamically import files from the filesystem at runtime. the binary contains the rhachet CLI; user configs stay external.

**with node SEA**: this is problematic — SEA has no native `require()` and limited dynamic import support.

#### 4. native modules check

if any dependency uses native node bindings (`.node` files), they won't work in statically-linked binaries:

```
@anthropic-ai/sdk     — pure JS ✓
commander             — pure JS ✓
zod                   — pure JS ✓
fast-glob             — pure JS ✓
```

no native modules detected — **binary compilation is viable**.

#### 5. debugging difficulty

stack traces point to bundled locations, not original source. mitigated by source maps but adds friction.

### dual binary distribution strategy

support both fast binary (default) and JIT fallback via symlink:

```
bin/
  run           # symlink → run.bun (default entry)
  run.bun       # compiled bun binary (fast)
  run.jit       # node-based JIT entry (fallback/debug)
```

benefits of symlink approach:
- easy to switch default implementation by updating symlink target
- clear naming distinguishes compiled vs JIT
- `bin/run` remains the stable entrypoint for package.json

package.json configuration:

```json
{
  "bin": {
    "rhachet": "./bin/run"
  },
  "scripts": {
    "postinstall": "node scripts/install-binary.js || true",
    "build:binary": "bun build ./src/contract/cli/invoke.ts --compile --bytecode --outfile bin/run.bun && ln -sf run.bun bin/run"
  }
}
```

the `postinstall` script would:
1. detect platform (linux-x64, darwin-arm64, etc)
2. download prebuilt binary to `bin/run.bun` from github releases
3. create symlink `bin/run` → `bin/run.bun`
4. if binary unavailable, symlink `bin/run` → `bin/run.jit` instead

### usage after implementation

```bash
npx rhachet run --skill X          # uses bin/run → bin/run.bun (fast)
./bin/run.jit run --skill X        # explicit JIT fallback for debugging
./bin/run.bun run --skill X        # explicit bun binary
```

### conclusion

binary compilation via `bun build --compile --bytecode` is recommended as the maximal solution:
- achieves ~38ms startup (well under 100ms target)
- no native module blockers
- dynamic config loading still works
- distribution complexity is solvable via postinstall pattern
