# research: remote access for brain-continue

research on remote repositories (apis, sdks) required to fulfill episode continuation for BrainAtom and BrainRepl.

---

## summary

| supplier | api | session model | continuation mechanism |
|----------|-----|---------------|------------------------|
| anthropic | messages api | stateless | client sends full messages array [1][2] |
| anthropic | agent sdk | server-side (unstable) | `unstable_v2_createSession`, `unstable_v2_resumeSession` [3] |
| openai | chat completions | stateless | client sends full messages array [4][5] |
| openai | responses api | server-side | `previous_response_id` + `store: true` [6][7][8] |
| openai | conversations api | server-side | `conversation_id` for persistent state [9] |
| google | gemini api | stateless | client sends full messages array [10] |
| google | interactions api (beta) | server-side | `previous_interaction_id` [11] |

---

## lesson.1: anthropic messages api is stateless

the core anthropic api has no server-side session management.

### contract

```
POST /v1/messages

request:
  model: string (required)
  max_tokens: number (required)
  messages: MessageParam[] (required)
  system?: string
  ...

response:
  id: string          # identifies this response only, not a session
  type: "message"
  role: "assistant"
  content: ContentBlock[]
  stop_reason: StopReason
  usage: { input_tokens, output_tokens }
```

### citations

[1] "The Messages API is stateless, which means that you always send the full conversational history to the API."
— [Anthropic Messages API](https://platform.claude.com/docs/en/api/messages)

[2] "Our models are trained to operate on alternating user and assistant conversational turns. When creating a new Message, you specify the prior conversational turns with the messages parameter, and the model then generates the next Message in the conversation."
— [Anthropic Messages API](https://platform.claude.com/docs/en/api/messages)

### implication

- no idempotency keys
- no session/conversation ids returned
- client must maintain full message history
- client must send full history with each request

---

## lesson.2: anthropic agent sdk has unstable session apis

the claude agent sdk (typescript/python) provides session management, but marked unstable.

### citations

[3] "Multi-turn conversations maintain session state across multiple message exchanges via the v2 session APIs (unstable_v2_createSession, unstable_v2_resumeSession)."
— [GitHub: anthropics/claude-agent-sdk-typescript](https://github.com/anthropics/claude-agent-sdk-typescript)

### implication

- session apis exist but are unstable (api may change)
- provides server-side state for multi-turn
- not yet stable for production reliance

---

## lesson.3: openai chat completions api is stateless

the original openai chat api follows the same stateless pattern as anthropic.

### contract

```
POST /v1/chat/completions

request:
  model: string (required)
  messages: Message[] (required)
  store?: boolean
  ...

response:
  id: string          # "chatcmpl-..." identifies this completion only
  object: "chat.completion"
  model: string
  choices: Choice[]
  usage: { prompt_tokens, completion_tokens, total_tokens }
```

### citations

[4] "The Chat Completions API endpoint will generate a model response from a list of messages that comprise a conversation."
— [OpenAI Chat Completions](https://platform.openai.com/docs/api-reference/chat)

[5] "Previously when you used the Chat Completion API, to support multi-turn conversation, developers would maintain the state of the conversation in their app and replay the 'assistant' messages."
— [OpenAI Developer Community](https://community.openai.com/t/multiple-messages-per-turn-in-chat-completions-api/670944)

### implication

- same pattern as anthropic: client-managed history
- `store` parameter exists for opt-in storage
- `id` in response is per-completion, not per-conversation

---

## lesson.4: openai responses api supports server-side state

the newer responses api (march 2025) provides `previous_response_id` for server-managed continuation.

### contract

```
POST /v1/responses

request:
  model: string (required)
  input: string | InputItem[] (required)
  previous_response_id?: string    # for continuation
  store?: boolean                  # enable persistence
  instructions?: string
  ...

response:
  id: string                       # can be used as previous_response_id
  object: "response"
  output: OutputItem[]
  usage: { ... }
```

### citations

[6] "The `previous_response_id` is the unique ID of the previous response to the model, used to create multi-turn conversations."
— [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses/create)

[7] "For multi-turn conversations that use the Responses API with `store=true`, subsequent requests should include the `previous_response_id` to maintain context."
— [OpenAI Conversation State Guide](https://platform.openai.com/docs/guides/conversation-state)

[8] "The Responses API has compatibility with the Conversations API for persistent conversations, or the ability to pass a `previous_response_id` to easily chain Responses together."
— [OpenAI Migration Guide](https://platform.openai.com/docs/guides/migrate-to-responses)

### implication

- server-side state management available
- `previous_response_id` enables chain without resend of history
- `store: true` required for persistence
- `id` in response can be used for continuation

---

## lesson.5: openai conversations api provides explicit containers

the conversations api provides explicit conversation containers.

### citations

[9] "The Conversations API includes endpoints like `GET /v1/conversations/{conversation_id}/items/{item_id}` where you specify the ID of the conversation that contains the item."
— [OpenAI Conversations API](https://platform.openai.com/docs/api-reference/conversations/create)

### implication

- explicit conversation management
- items can be added/retrieved by conversation_id
- more control than `previous_response_id` approach

---

## lesson.6: google gemini api is stateless at core

the gemini api follows the same stateless pattern.

### citations

[10] "The Gemini API is stateless, which raises questions about how to manage conversation history effectively."
— [Google AI Developers Forum](https://discuss.ai.google.dev/t/maintaining-session-for-a-chat-conversation-in-gemini-api-multi-turn-conversations/76418)

### implication

- same pattern: client must manage history
- sdk provides convenience via `startChat()` / `sendMessage()` but state is client-side

---

## lesson.7: google interactions api (beta) supports server-side state

the newer interactions api provides server-side continuation.

### citations

[11] "You can use the id of a completed interaction in a subsequent call with the `previous_interaction_id` parameter to continue the conversation. The server uses this ID to retrieve the conversation history, which saves you from the need to resend the entire chat history."
— [Google Interactions API](https://ai.google.dev/gemini-api/docs/interactions)

### implication

- server-side state available (beta)
- similar pattern to openai's `previous_response_id`
- reduces payload size for long conversations

---

## lesson.8: two continuation strategies exist

across all suppliers, two patterns emerge:

### strategy.1: client-managed history (stateless)

```
request 1: messages = [user1]
response 1: assistant1

request 2: messages = [user1, assistant1, user2]
response 2: assistant2

request 3: messages = [user1, assistant1, user2, assistant2, user3]
response 3: assistant3
```

- client stores full history
- client sends full history each time
- works with all suppliers
- no server-side state dependency

### strategy.2: server-managed history (stateful)

```
request 1: input = user1
response 1: { id: "resp_123", output: assistant1 }

request 2: previous_response_id = "resp_123", input = user2
response 2: { id: "resp_456", output: assistant2 }

request 3: previous_response_id = "resp_456", input = user3
response 3: { id: "resp_789", output: assistant3 }
```

- server stores history
- client sends only `previous_*_id` + new input
- reduces payload size
- depends on server-side state (availability, retention)

---

## lesson.9: rhachet current contract is stateless

the current BrainAtom and BrainRepl interfaces have no episode continuation.

### current contract (from src/domain.objects/)

```typescript
// BrainAtom.ask
ask: <TOutput>(input: {
  plugs?: BrainAtomPlugs;
  role: { briefs?: Artifact<typeof GitFile>[] };
  prompt: string;
  schema: { output: z.Schema<TOutput> };
}, context?: Empty) => Promise<BrainOutput<TOutput>>;

// BrainRepl.ask / BrainRepl.act
ask: <TOutput>(input: { ... }) => Promise<BrainOutput<TOutput>>;
act: <TOutput>(input: { ... }) => Promise<BrainOutput<TOutput>>;
```

### implication

- no `messages` array in current contract
- no episode reference in input or output
- each call is independent
- need to extend contract for episode continuation

---

## recommendation: abstraction over both strategies

rhachet should abstract over both continuation strategies:

### rhachet episode contract (proposed)

```typescript
// start fresh episode
const { episode, output } = await brain.ask({
  prompt: "...",
  ...
});

// continue prior episode
const { episode, output } = await brain.ask({
  in: { episode },
  prompt: "...",
  ...
});
```

### internal implementation

rhachet can implement episode continuation via:

1. **client-managed history** (strategy.1)
   - store messages in `BrainEpisode` object
   - send full history to stateless apis
   - works with all suppliers

2. **server-managed history** (strategy.2)
   - store `previous_response_id` in `BrainEpisode` object
   - use supplier's server-side state when available
   - fallback to client-managed for suppliers without server state

the abstraction hides the implementation detail from callers.

---

## sources

| # | source | url |
|---|--------|-----|
| 1 | Anthropic Messages API | https://platform.claude.com/docs/en/api/messages |
| 2 | Anthropic Messages API | https://platform.claude.com/docs/en/api/messages |
| 3 | Claude Agent SDK | https://github.com/anthropics/claude-agent-sdk-typescript |
| 4 | OpenAI Chat Completions | https://platform.openai.com/docs/api-reference/chat |
| 5 | OpenAI Developer Forum | https://community.openai.com/t/multiple-messages-per-turn-in-chat-completions-api/670944 |
| 6 | OpenAI Responses API | https://platform.openai.com/docs/api-reference/responses/create |
| 7 | OpenAI Conversation State | https://platform.openai.com/docs/guides/conversation-state |
| 8 | OpenAI Migration Guide | https://platform.openai.com/docs/guides/migrate-to-responses |
| 9 | OpenAI Conversations API | https://platform.openai.com/docs/api-reference/conversations/create |
| 10 | Google AI Forum | https://discuss.ai.google.dev/t/maintaining-session-for-a-chat-conversation-in-gemini-api-multi-turn-conversations/76418 |
| 11 | Google Interactions API | https://ai.google.dev/gemini-api/docs/interactions |
