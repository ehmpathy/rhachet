# research claims: briefs compression for LLM agent consumption

## 1. Telegraphic Semantic Compression (TSC)

### compression ratios and token savings

[1] [FACT] TSC on the Eiffel Tower sentence reduced tokens from 26 to 18, a 30.8% reduction. On the Amazon Rainforest passage, tokens went from 122 to 73, a 40.2% reduction.
- Source: Nuno Bispo, "Telegraphic Semantic Compression (TSC) - A Semantic Compression Method for LLM Contexts", Nov 2025
- URL: https://dev.to/devasservice/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts-1akl

[2] [FACT] TSC example — Original: "The Eiffel Tower, located in Paris, France, was built in 1889 for the Exposition Universelle." Compressed: "Eiffel Tower locate Paris France build 1889 Exposition Universelle."
- Source: ibid [1]

[3] [SUMP] "Token savings are often 30-50%, sometimes more for longer passages." This is stated as general guidance but not backed by a large-scale benchmark; it is an assumption derived from a handful of examples.
- Source: ibid [1]

[4] [SUMP] TSC "produces dramatically denser and more efficient prompts without sacrifice of the factual integrity of the original text." No formal evaluation of factual integrity loss is provided.
- Source: ibid [1]

[5] [OPIN] TSC exploits the claim that "Large Language Models excel at prediction and reconstruction of natural language, so removal of the predictable grammatical elements doesn't harm comprehension while it significantly reduces token usage." No controlled experiment with LLM task accuracy is cited.
- Source: ibid [1]

[6] [KHUE] Does TSC degrade LLM performance on nuanced instructions (e.g., enforcement levels, edge cases, conditional logic) compared to full prose? No published benchmark tests this on instruction-follow tasks.

### method characteristics

[7] [FACT] TSC removes: articles ("the", "a"), prepositions ("of", "in"), auxiliary verbs ("was", "is"), filler words ("just", "really", "basically"), conjunctions where context is clear, pronouns when referent is obvious. It retains: nouns, verbs, numbers, entity names, domain-specific vocabulary.
- Source: ibid [1]

[8] [FACT] TSC implementation uses spaCy for POS tag extraction and tiktoken for token count measurement. Code is available at https://github.com/nunombispo/TelegraphicSemanticCompression-Article
- Source: ibid [1]

---

## 2. LLMLingua / LongLLMLingua / LLMLingua-2 (Microsoft Research)

### LLMLingua (EMNLP 2023)

[9] [FACT] LLMLingua achieves "up to 20x compression while it preserves the original prompt's capabilities" with "only a 1.5-point performance loss" on GSM8K.
- Source: Microsoft Research Blog, "LLMLingua: Innovation in LLM efficiency with prompt compression"
- URL: https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/

[10] [FACT] LLMLingua accelerates end-to-end inference by 1.7x-5.7x on GSM8K. At 10x token compression, acceleration reaches up to 5.7x.
- Source: ibid [9]

[11] [FACT] LLMLingua reduces response generation length by 20 to 30 percent (fewer output tokens generated).
- Source: ibid [9]

[12] [FACT] "GPT-4 successfully recovered all nine chain-of-thought steps from compressed prompts with preserved meaning."
- Source: ibid [9]

[13] [FACT] On GSM8K, LLMLingua with GPT-2-small scored 76.27 vs standard 74.9; with LLaMA-7B scored 77.33. On BBH, Claude-v1.3 scored 82.61 vs standard 81.8.
- Source: ibid [9]

### LongLLMLingua (ACL 2024)

[14] [FACT] "LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo" on the NaturalQuestions benchmark.
- Source: ACL 2024, "LongLLMLingua: Acceleration and Enhancement of LLMs in Long Context Scenarios via Prompt Compression"
- URL: https://aclanthology.org/2024.acl-long.91/

[15] [FACT] LongLLMLingua accelerates end-to-end latency by 1.4x-2.6x when it compresses prompts of ~10k tokens at 2x-6x ratios.
- Source: ibid [14]

### LLMLingua-2 (ACL 2024 Findings)

[16] [FACT] LLMLingua-2 is "3x-6x faster than prior prompt compression methods, while it accelerates end-to-end latency by 1.6x-2.9x with compression ratios of 2x-5x."
- Source: "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression", ACL 2024 Findings
- URL: https://arxiv.org/abs/2403.12968

[17] [FACT] LLMLingua-2 uses XLM-RoBERTa fine-tuned on GPT-4-generated data and achieves "95-98% accuracy retention" at 3-6x compression.
- Source: ibid [16]

[18] [FACT] LLMLingua-2 trained only on MeetingBank data generalizes effectively to out-of-domain datasets (LongBench), "with performance comparable to or surpassed by state-of-the-art task-agnostic compression baselines."
- Source: ibid [16]

### tradeoffs

[19] [SUMP] LLMLingua requires model inference at build time — a small language model must run to score token importance. This introduces a tool dependency and non-determinism compared to rule-based approaches like TSC.
- Source: architectural observation from https://llmlingua.com/

[20] [KHUE] Is the LLMLingua compression model suitable for compression of instruction/brief-style text (rules, constraints, enforcement levels), or is it primarily optimized for factual/QA content?

---

## 3. Symbolic Metalanguages / MetaGlyph

[21] [FACT] MetaGlyph achieves "62-81% token reduction across all task types." Selection tasks compress most at 80.9%; conditional transformation tasks compress least at 62.2%.
- Source: Ernst van Gassen, "Semantic Compression of LLM Instructions via Symbolic Metalanguages", arXiv:2601.07354, Jan 2026
- URL: https://arxiv.org/abs/2601.07354

[22] [FACT] MetaGlyph uses mathematical symbols (∈, ⇒, ¬, etc.) that "models already understand from their train data" — no explicit decode rules are needed.
- Source: ibid [21]

[23] [FACT] Control prompts with meaningless symbols produced 0% equivalence, "confirmed: models respond to symbolic meaning, not just appearance."
- Source: ibid [21]

### model-specific performance

[24] [FACT] Kimi K2 reaches 98.1% fidelity for the implication operator (⇒) and achieves 100% accuracy on selection tasks with symbolic prompts, which exceeds prose performance at 90.8%.
- Source: ibid [21]

[25] [FACT] GPT-5.2 Chat shows the highest membership fidelity observed (91.3%) but with "0% parse success on complex tasks" and "variable parse success across task types."
- Source: ibid [21]

[26] [FACT] Claude Haiku 4.5 achieves 100% parse success across all tasks but only 26% membership operator fidelity.
- Source: ibid [21]

[27] [FACT] Gemini 2.5 Flash achieves 75% semantic equivalence on selection tasks with 49.9% membership operator fidelity.
- Source: ibid [21]

[28] [FACT] Mid-sized open-source models (7B-12B) show "near-zero operator fidelity, which indicates instruction-tune biases."
- Source: ibid [21]

### limitations

[29] [FACT] The research was "intentionally narrow: four task families, eight models that span 3B to 1T parameters." Conjunction operators show unreliable performance across models.
- Source: ibid [21]

[30] [OPIN] MetaGlyph is best suited for structured/logical constraints rather than prose guidance. The high variance across models and operators suggests it is not yet reliable for general-purpose instruction compression.
- Source: synthesis of ibid [21]

[31] [KHUE] Would a hybrid approach (TSC for prose + MetaGlyph symbols for logical constraints like enforcement levels) yield better compression than either alone?

---

## 4. General LLM Prompt Compression — Do LLMs Understand Compressed English?

[32] [FACT] The LLMLingua project page states: "natural language is redundant with amount of information that varies, LLMs can understand compressed prompt, and there is a trade-off between language completeness and compression ratio."
- Source: https://llmlingua.com/

[33] [SUMP] The assumption that LLMs "reconstruct" removed grammatical elements is widely held but not formally proven for all task types. Most benchmarks test factual QA and chain-of-thought, not instruction-follow fidelity.

[34] [KHUE] At what compression ratio does instruction-follow performance degrade meaningfully? The 20x claim from LLMLingua is demonstrated on math/logic tasks (GSM8K, BBH), not on "follow these code rules" style tasks.

[35] [OPIN] The absence of benchmarks for compressed-instruction-follow suggests the field assumes LLM comprehension of compressed prompts generalizes across task types. This assumption is untested for brief-style instructions.

---

## 5. Context Window Management — System Prompt Impact

### lost in the middle

[36] [FACT] Liu et al. (TACL 2024) found that "performance can degrade by more than 30% when relevant information shifts from the start or end positions to the middle of the context window." A "distinctive U-shaped performance curve" was observed.
- Source: Liu, Nelson F. et al., "Lost in the Middle: How Language Models Use Long Contexts", Transactions of the Association for Computational Linguistics, 2024
- URL: https://aclanthology.org/2024.tacl-1.9/

[37] [FACT] The U-shaped curve shows "language model performance is highest when relevant information occurs at the very start (primacy bias) or end of its input context (recency bias), and performance significantly degrades when models must access relevant information in the middle."
- Source: ibid [36]

### context rot

[38] [FACT] Chroma Research's "Context Rot" study found that "models do not use their context uniformly; instead, their performance grows increasingly unreliable as input length grows." Tests covered 18 LLMs that include GPT-4.1, Claude 4, Gemini 2.5, and Qwen3.
- Source: Chroma Research, "Context Rot: How Increased Input Tokens Impact LLM Performance"
- URL: https://research.trychroma.com/context-rot

[39] [FACT] Counterintuitively, "models perform worse when the haystack preserves a logical flow of ideas. Shuffled haystack with removed local coherence consistently improves performance."
- Source: ibid [38]

[40] [FACT] In LongMemEval results, "substantial gaps appear between focused (~300 tokens) and full (~113k tokens) prompts." Claude models show "pronounced abstention under ambiguity"; GPT models exhibit "higher hallucination rates."
- Source: ibid [38]

### context length degrades performance even with perfect retrieval

[41] [FACT] Du, Tian, Ronanki et al. demonstrate that "performance still degrades substantially (13.9%-85%) as input length increases" even when all irrelevant tokens are masked and the model attends only to evidence and question.
- Source: "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval", arXiv:2510.05381, Oct 2025
- URL: https://arxiv.org/abs/2510.05381

[42] [FACT] Specific degradation at 30K tokens: Llama-3.1-8B drops 24.2% on MMLU, 59% on Variable Summation, 47.6% on HumanEval. Claude-3.5 drops 67.6% on MMLU at 30K tokens.
- Source: ibid [41]

[43] [FACT] Even closed-source models (GPT-4o, Claude-3.5, Gemini-2.0) show "substantial and mostly consistent degradation" with increased context length despite perfect retrieval.
- Source: ibid [41]

### system prompt specific impact

[44] [SUMP] "A large system prompt reduces how much history you can keep in the conversation." and "Long system prompts mostly hurt the prefill phase, which slows time to first token and bloats the KV cache for the rest of the turn."
- Source: Lucas Valbuena, "Why Long System Prompts Hurt Context Windows (and How to Fix It)", Medium
- URL: https://medium.com/data-science-collective/why-long-system-prompts-hurt-context-windows-and-how-to-fix-it-7a3696e1cdf9

[45] [FACT] "Input tokens are typically billed for remote calls, which means an increase in context provided directly increases query costs. Additionally, research demonstrates that more input tokens generally leads to slower output token generation."
- Source: Meibel, "How Increased LLM Context Windows Affect Performance"
- URL: https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows

[46] [KHUE] If 58k tokens of briefs sit at the start of context, and the "lost in the middle" effect places a U-shaped attention curve, does this mean briefs content is actually well-positioned (at the start)? Or does it push task-relevant content (user messages, code) into the degraded middle zone?

[47] [KHUE] Per claim [41], context length alone degrades performance. Does reduction of brief size from 58k to ~29k tokens yield a measurable improvement in downstream task accuracy, independent of cost savings?

---

## 6. Token Cost Economics

### price landscape (as of early 2025)

[48] [FACT] Claude Opus 4.1: $15.00 input / $75.00 output per 1M tokens. Claude Sonnet 4: $3.00 input / $15.00 output per 1M tokens. Claude Haiku 3.5: $0.80 input / $4.00 output per 1M tokens.
- Source: IntuitionLabs, "LLM API Price Comparison 2025"
- URL: https://intuitionlabs.ai/articles/llm-api-pricing-comparison-2025

[49] [FACT] GPT-5: $1.25 input / $10.00 output per 1M tokens. GPT-4.1: $3.00 input / $12.00 output per 1M tokens. GPT-4o: $5.00 input / $20.00 output per 1M tokens.
- Source: ibid [48]

[50] [FACT] Gemini 2.5 Pro (under 200K input): $1.25 input / $10.00 output per 1M tokens. Gemini 2.5 Flash: $0.15 input / $0.60 output per 1M tokens.
- Source: ibid [48]

[51] [FACT] "Identical tasks could cost anywhere from a few cents to hundreds of dollars that depend on provider and model selection."
- Source: ibid [48]

### prompt cache economics

[52] [FACT] Anthropic prompt cache: cache write tokens cost 1.25x base input price (5-min TTL) or 2x base input price (1-hour TTL). Cache read tokens cost 0.1x base input price — a 90% discount on cached reads.
- Source: Anthropic, "Prompt cache" documentation
- URL: https://platform.claude.com/docs/en/build-with-claude/prompt-caching

[53] [FACT] Anthropic claims prompt cache "can reduce latency by up to 85% for long prompts."
- Source: ibid [52]

[54] [FACT] Example: Claude Sonnet at $3/M input — first request (cache write, 5-min): 5,000 tokens x $3.75/M = $0.01875. Subsequent requests (cache read): 5,000 tokens x $0.30/M = $0.00150. A 92% cost reduction on cached reads.
- Source: ngrok blog, "Prompt cache: 10x cheaper LLM tokens, but how?"
- URL: https://ngrok.com/blog/prompt-caching/

[55] [FACT] One practitioner reported a shift "from $720 to $72 monthly on API costs" with prompt cache — a 90% reduction.
- Source: Du'An Lightfoot, "Prompt Cache is a Must!", Medium
- URL: https://medium.com/@labeveryday/prompt-caching-is-a-must-how-i-went-from-spending-720-to-72-monthly-on-api-costs-3086f3635d63

### cost impact of brief compression at scale

[56] [SUMP] If 58k tokens of briefs are loaded per session and compressed to ~29k tokens (50% reduction), the per-session savings on Claude Sonnet input: 29,000 tokens x $3.00/M = $0.087 per session. At 1,000 sessions/day = $87/day = ~$2,600/month saved on input tokens alone.

[57] [SUMP] With prompt cache active (0.1x read price), the same 29k token reduction saves: 29,000 x $0.30/M = $0.0087 per cached session. At 1,000 sessions/day = $8.70/day = ~$261/month. Prompt cache already captures most savings, so compression's marginal cost benefit is smaller when cache is in play.

[58] [KHUE] Is the primary value of brief compression cost reduction, or is it the performance benefit from shorter context (per claims [41]-[43])? If cache already handles cost, compression's main value may be quality/speed, not dollars.

[59] [OPIN] Compression and cache are complementary, not in competition. Compression reduces the size of what gets cached, which further reduces cache write costs and KV-cache memory pressure. Even with cache, fewer tokens = faster prefill = lower latency to first token.

---

## 7. Cross-Cut Synthesis

[60] [KHUE] The brief research document estimates 58k tokens at session start, which consumes 29% of a 200k context window. Per claim [41] (performance degrades 13.9%-85% with context length even under perfect retrieval), what is the actual performance cost of those 58k tokens on downstream code tasks?

[61] [KHUE] TSC is validated only on factual/descriptive text (Eiffel Tower, Amazon Rainforest passages). Brief content includes enforcement levels (BLOCKER, WARNING), conditional logic, code examples, and pattern demonstrations. Does TSC preserve these distinctions?

[62] [OPIN] The safest compression approach for briefs is a layered strategy: (1) structural deduplication and trim first (deterministic, lossless), (2) TSC-style grammatical strip second (deterministic, lossy but auditable), (3) optional symbolic compression for logical constraints only if validated. LLMLingua-style model-based compression adds tool complexity for marginal gains over TSC on instruction text.

[63] [OPIN] A quality gate is essential: compressed briefs should be tested against uncompressed briefs on a representative set of code tasks to measure comprehension loss before deployment. No benchmark covers this use case today.
