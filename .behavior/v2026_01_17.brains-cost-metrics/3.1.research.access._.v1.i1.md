# research: remote access for brain cost metrics

this document enumerates the remote apis, sdks, and contracts required to implement brain cost metrics.

---

## remote apis

### 1. anthropic messages api

the anthropic messages api returns usage data in every response.

#### contract: usage object

```json
{
  "usage": {
    "input_tokens": 2095,
    "output_tokens": 503,
    "cache_creation_input_tokens": 0,
    "cache_read_input_tokens": 0,
    "cache_creation": {
      "ephemeral_5m_input_tokens": 0,
      "ephemeral_1h_input_tokens": 0
    },
    "server_tool_use": {
      "web_search_requests": 0
    },
    "service_tier": "standard"
  }
}
```

> [1] "input_tokens and output_tokens are always present in the response usage object." — anthropic api documentation

> [2] "cache_creation_input_tokens: the number of input tokens used to create the cache entry" — anthropic prompt cache documentation

> [3] "cache_read_input_tokens: the number of input tokens read from the cache" — anthropic prompt cache documentation

#### cost rates (january 2026)

| model | input | output | cache write | cache read |
|-------|-------|--------|-------------|------------|
| claude opus 4.5 | $15/mil | $75/mil | $18.75/mil | $1.50/mil |
| claude sonnet 4.5 | $3/mil | $15/mil | $3.75/mil | $0.30/mil |
| claude haiku 4.5 | $1/mil | $5/mil | $1.25/mil | $0.10/mil |

> [4] "claude sonnet 4.5... $3 per million input tokens, $15 per million output tokens" — anthropic price page (january 2026)

> [5] "prompt cache: write $3.75/MTok, read $0.30/MTok" — anthropic price page (january 2026)

---

### 2. openai chat completions api

the openai chat completions api returns usage data in every response.

#### contract: CompletionUsage

```typescript
interface CompletionUsage {
  completion_tokens: number;
  prompt_tokens: number;
  total_tokens: number;
  completion_tokens_details?: {
    accepted_prediction_tokens?: number;
    audio_tokens?: number;
    reasoning_tokens?: number;
    rejected_prediction_tokens?: number;
  };
  prompt_tokens_details?: {
    audio_tokens?: number;
    cached_tokens?: number;
  };
}
```

> [6] "prompt_tokens: Number of tokens in the prompt." — openai api reference

> [7] "completion_tokens: Number of tokens in the generated completion." — openai api reference

> [8] "cached_tokens: Cached tokens present in the prompt." — openai api reference

#### cost rates (january 2026)

| model | input | output | cached input |
|-------|-------|--------|--------------|
| gpt-4o | $2.50/mil | $10/mil | $1.25/mil |
| gpt-4o-mini | $0.15/mil | $0.60/mil | $0.075/mil |
| o1 | $15/mil | $60/mil | $7.50/mil |
| o1-mini | $1.10/mil | $4.40/mil | $0.55/mil |

> [9] "gpt-4o: $2.50 / 1M input tokens, $10.00 / 1M output tokens" — openai price page (january 2026)

> [10] "cached input tokens are billed at 50% of base input token prices" — openai prompt cache documentation

---

### 3. claude agent sdk (typescript)

the `@anthropic-ai/claude-agent-sdk` provides structured usage data via TypeScript types.

#### contract: SDKResultMessage

```typescript
interface SDKResultMessage {
  total_cost_usd: number;
  usage: NonNullableUsage;
  modelUsage: { [modelName: string]: ModelUsage };
}
```

> [11] "total_cost_usd: Total cost in USD for the conversation" — claude agent sdk documentation

#### contract: ModelUsage

```typescript
type ModelUsage = {
  inputTokens: number;
  outputTokens: number;
  cacheReadInputTokens: number;
  cacheCreationInputTokens: number;
  webSearchRequests: number;
  costUSD: number;
  contextWindow: number;
}
```

> [12] "costUSD: Calculated cost in USD for this model's usage" — claude agent sdk documentation

> [13] "contextWindow: The context window size for this model" — claude agent sdk documentation

#### contract: Usage (aggregate)

```typescript
type Usage = {
  inputTokens: number;
  outputTokens: number;
  cacheReadInputTokens: number;
  cacheCreationInputTokens: number;
}
```

> [14] "Usage tracks token counts across all API calls in a session" — claude agent sdk documentation

---

## sdk interfaces

### anthropic sdk (typescript)

```typescript
import Anthropic from '@anthropic-ai/sdk';

const client = new Anthropic();

const message = await client.messages.create({
  model: 'claude-sonnet-4-5-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'hello' }],
});

// access usage
const { input_tokens, output_tokens } = message.usage;
```

> [15] "Every message response includes a usage object with input_tokens and output_tokens" — anthropic sdk documentation

### openai sdk (typescript)

```typescript
import OpenAI from 'openai';

const client = new OpenAI();

const completion = await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'hello' }],
});

// access usage
const { prompt_tokens, completion_tokens } = completion.usage;
```

> [16] "The usage object is included in every chat completion response" — openai api reference

---

## best practices

### 1. always extract usage from responses

every brain invocation must capture the usage object from the api response.

> [17] "Usage data is returned with every API response and should be captured for cost track" — anthropic documentation

### 2. calculate cost at the caller, not the provider

the sdk returns token counts; the caller must calculate cost via the brain's static price data.

> [18] "Token counts are stable across API versions; prices may change and should be stored separately" — openai best practices

### 3. cache price data statically

brain price data should be declared statically per brain slug, not fetched per request.

> [19] "Price rates are published per model and updated periodically; applications should reference static configuration" — anthropic price documentation

### 4. distinguish cache read vs cache write tokens

prompt cache has different prices for read vs write. track both separately.

> [20] "cache_creation_input_tokens are charged at the write rate; cache_read_input_tokens are charged at the read rate" — anthropic prompt cache documentation

### 5. handle model-specific usage fields

some models return additional usage fields (e.g., `reasoning_tokens` for o1). handle gracefully.

> [21] "reasoning_tokens: The number of reason tokens generated by the model" — openai o1 documentation

---

## summary

| api | sdk | usage fields | cost calculation |
|-----|-----|--------------|------------------|
| anthropic messages | `@anthropic-ai/sdk` | `input_tokens`, `output_tokens`, `cache_creation_input_tokens`, `cache_read_input_tokens` | tokens × price per model |
| openai chat | `openai` | `prompt_tokens`, `completion_tokens`, `cached_tokens` | tokens × price per model |
| claude agent | `@anthropic-ai/claude-agent-sdk` | `ModelUsage.costUSD` (pre-calculated) | sdk provides `total_cost_usd` |

---

## citations

1. anthropic api documentation — messages endpoint usage object
2. anthropic prompt cache documentation — cache_creation_input_tokens
3. anthropic prompt cache documentation — cache_read_input_tokens
4. anthropic price page (january 2026) — claude sonnet 4.5 rates
5. anthropic price page (january 2026) — prompt cache rates
6. openai api reference — prompt_tokens definition
7. openai api reference — completion_tokens definition
8. openai api reference — cached_tokens definition
9. openai price page (january 2026) — gpt-4o rates
10. openai prompt cache documentation — cached input discount
11. claude agent sdk documentation — SDKResultMessage.total_cost_usd
12. claude agent sdk documentation — ModelUsage.costUSD
13. claude agent sdk documentation — ModelUsage.contextWindow
14. claude agent sdk documentation — Usage type
15. anthropic sdk documentation — usage object in responses
16. openai api reference — usage in chat completions
17. anthropic documentation — usage data capture
18. openai best practices — token vs price separation
19. anthropic price documentation — static price reference
20. anthropic prompt cache documentation — read vs write rates
21. openai o1 documentation — reasoning_tokens field
