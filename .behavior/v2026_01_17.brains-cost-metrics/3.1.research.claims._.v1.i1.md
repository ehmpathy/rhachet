# research: claims for brain cost metrics

this document enumerates facts, assumptions, questions, and opinions relevant to the brain cost metrics feature.

---

## token-based cost model

### [FACT] input tokens cost less than output tokens

> [1] "Input tokens are generally less expensive than output tokens because the model only needs to process and understand them, not generate new content. Output tokens are the text generated by the model in response to your input. These usually cost two to five times more than input tokens." — [mobisoft](https://mobisoftinfotech.com/resources/blog/ai-development/llm-api-pricing-guide)

> [2] "For example, with Claude Sonnet 4.5, input tokens cost $3 per million while output tokens cost $15 per million, a 5:1 ratio." — [mobisoft](https://mobisoftinfotech.com/resources/blog/ai-development/llm-api-pricing-guide)

### [FACT] usage data is returned with every api response

> [3] "Every message response includes a usage object with input_tokens and output_tokens" — [anthropic sdk documentation](https://docs.anthropic.com)

> [4] "The usage object is included in every chat completion response" — [openai api reference](https://platform.openai.com/docs/api-reference)

### [FACT] cache tokens have separate rates

> [5] "cache_creation_input_tokens are charged at the write rate; cache_read_input_tokens are charged at the read rate" — [anthropic prompt cache documentation](https://docs.anthropic.com/en/docs/build-with-claude/prompt-caching)

> [6] "cached input tokens are billed at 50% of base input token prices" — [openai prompt cache documentation](https://platform.openai.com/docs/guides/prompt-caching)

### [FACT] prices change frequently

> [7] "Notably, 116 out of 296 tracked models have had a price change in January 2026, highlighting the importance of keeping pricing information up-to-date." — [aimultiple](https://research.aimultiple.com/llm-pricing/)

---

## hidden costs

### [FACT] hidden costs account for 20-40% of total llm operational expenses

> [8] "Beyond per-token pricing, production deployments incur additional costs including embeddings and vector databases, logging, monitoring, and auditing. Enterprises often incur costs for token-level monitoring, latency tracking, and security audits. These hidden costs often account for 20–40% of total LLM operational expenses." — [helicone](https://www.helicone.ai/blog/monitor-and-optimize-llm-costs)

### [SUMP] token usage alone is sufficient for cost calculation

this assumption is implicit in most cost calculation apis. however, claim [8] suggests hidden costs exist beyond tokens.

---

## observability best practices

### [FACT] core metrics to track

> [9] "Track tokens per request, cost per request, request frequency by endpoint, and cache hit rates as your core metrics." — [helicone](https://www.helicone.ai/blog/monitor-and-optimize-llm-costs)

### [OPIN] regular cost reviews should be monthly or quarterly

> [10] "Make LLM cost reviews a regular part of your workflow, ideally monthly or quarterly." — [helicone](https://www.helicone.ai/blog/monitor-and-optimize-llm-costs)

### [FACT] cost attribution requires metadata tag

> [11] "The most effective way to track costs per user is to pass metadata with every API request. By including a user_id in the metadata of an API call, you are permanently tagging that request (and its associated cost) to a specific user." — [traceloop](https://www.traceloop.com/blog/from-bills-to-budgets-how-to-track-llm-token-usage-and-cost-per-user)

---

## budget enforcement

### [FACT] budget limits can auto-prevent usage

> [12] "Budget limits allow you to set maximum LLM spending or token usage thresholds on your API keys, automatically preventing further usage when limits are reached." — [portkey](https://portkey.ai/docs/product/administration/enforce-budget-and-rate-limit)

### [FACT] guardrail budgets are enforced per-user and per-key

> [13] "Guardrail budgets are enforced per-user and per-key, not shared across all users with that guardrail. When an API key makes a request, its usage counts toward both the key's budget and the owning member's budget." — [openrouter](https://openrouter.ai/docs/guides/features/guardrails)

### [SUMP] budget enforcement happens before the request

some systems block requests that would exceed budget; others allow the request and enforce after. the vision assumes pre-enforcement.

### [KHUE] should budget enforcement be hard (block) or soft (warn)?

> [14] "Individual API key budgets still apply, and the lower limit wins." — [openrouter](https://openrouter.ai/docs/guides/features/guardrails)

---

## model selection and escalation

### [FACT] no single llm is optimal for all query types

> [15] "LLM routers address two fundamental limitations of single-model deployments: (1) no LLM is universally optimal across all query types and domains, and (2) high-performance models typically incur greater inference costs." — [emergentmind](https://www.emergentmind.com/topics/llm-routers)

### [FACT] cascade routers invoke models in order of increased cost

> [16] "Non-Predictive (Cascading) Routers sequentially invoke LLMs in order of increasing cost/performance; accept the first that passes an output 'judge' criterion. The method can exceed the individual LLMs in cost-effectiveness if the judge is highly reliable." — [emergentmind](https://www.emergentmind.com/topics/llm-routers)

### [FACT] intelligent route achieves 30-50% cost reduction

> [17] "Teams implementing intelligent routing report 30-50% cost reductions without measurable quality degradation when routing strategies align models to task requirements effectively." — [emergentmind](https://www.emergentmind.com/topics/llm-routers)

### [OPIN] cheaper model handles 90%+ of cases adequately

> [18] "Cost reduction with acceptable tradeoff: The cheaper model handles 90%+ of cases adequately. Cost savings justify the 10% degradation." — [futureagi](https://futureagi.com/blogs/llm-cost-optimization-2025)

### [FACT] 78% of enterprises use multi-model strategies

> [19] "Router patterns, abstraction layers, and fallback chains aren't over-engineering. They're production-grade architecture. 78% of enterprises use multi-model strategies for exactly this reason." — [getmaxim](https://www.getmaxim.ai/articles/5-ways-to-optimize-costs-and-latency-in-llm-powered-applications/)

---

## benchmark scores

### [FACT] mmlu is saturated at top models

> [20] "MMLU is quite saturated (top models score 88% or higher), making GPQA a better differentiator for frontier models." — [llm-stats](https://llm-stats.com/benchmarks)

> [21] "MMLU scores jumped from 70% in 2022 to over 90% in 2025." — [llm-stats](https://llm-stats.com/benchmarks)

### [FACT] swe-bench verified is the gold standard for code eval

> [22] "SWE-bench Verified is the current gold standard for coding evaluation. Unlike HumanEval, which tests isolated function writing, SWE-bench drops models into real GitHub repositories and asks them to fix actual bugs." — [llm-stats](https://llm-stats.com/benchmarks)

> [23] "As of December 2025, Claude Opus 4.5 leads at 80.9%." — [llm-stats](https://llm-stats.com/benchmarks/swe-bench-verified)

### [OPIN] no single score tells the whole story

> [24] "No single score tells the whole story." — [llm-stats](https://llm-stats.com/benchmarks)

### [KHUE] which benchmarks are most relevant for brain selection?

the vision mentions swe-bench, mmlu, and humaneval. should others be included?

---

## context window

### [FACT] claude models offer 200k tokens, with 1m beta

> [25] "While the older Claude 3 models featured a 200K-token context window, the Claude 4 models also offer an impressive 200K token window (with a beta 1 million token context window on Sonnet 4)." — [aimultiple](https://research.aimultiple.com/ai-context-window/)

### [FACT] gpt-4o has 128k context window

> [26] "OpenAI's GPT-4o boasts a 128,000 token context window, highly effective for handling long, complex documents, generating code, and performing document-based retrirhachet completion --setuptasks." — [aimultiple](https://research.aimultiple.com/ai-context-window/)

### [FACT] most models break before advertised limit

> [27] "Most models break much earlier than advertised. A model claiming 200k tokens typically becomes unreliable around 130k, with sudden performance drops rather than gradual degradation." — [chroma research](https://research.trychroma.com/context-rot)

### [KHUE] should gain.size.context report advertised or effective limit?

---

## tool use capability

### [FACT] berkeley function call leaderboard is the standard

> [28] "The Berkeley Function Calling Leaderboard (BFCL) V4 evaluates the LLM's ability to call functions (aka tools) accurately." — [berkeley](https://gorilla.cs.berkeley.edu/leaderboard.html)

### [FACT] top models achieve 95%+ tool use success rate

> [29] "GPT-5.2 (xhigh) and Gemini 3 Pro currently lead in function calling reliability, with 95%+ success rates on the IFBench benchmark. Claude Opus 4.5 excels at complex multi-tool orchestration and reasoning chains." — [whatllm](https://whatllm.org/blog/best-agentic-models-january-2026)

---

## api contract design

### [FACT] return type change is a break

> [30] "The return value of a method or the type of a property or field cannot be modified. For example, the signature of a method that returns an Object cannot be changed to return a String, or vice versa." — [microsoft](https://learn.microsoft.com/en-us/dotnet/core/compatibility/library-change-rules)

### [FACT] additive changes are non-break

> [31] "In most cases, a change that adds to an API is not a breaking change. This type of change, called an additive change, may involve new resources or methods, new response fields, and even new query parameters." — [nordicapis](https://nordicapis.com/what-are-breaking-changes-and-how-do-you-avoid-them/)

### [SUMP] output wrapper { output, metrics } is worth the break

the vision explicitly acknowledges this is a break but asserts the tradeoff is worth it.

### [OPIN] return value semantics change is a sneaky break

> [32] "Changing the meaning of a return value is absolutely a breaking change because clients depend on the behavior. This is a sneaky and subtle change, because preserving the method signature means that the change could go completely undetected by clients, and the effects could ripple through the system." — [bennadel](https://www.bennadel.com/blog/3501-when-is-a-change-a-breaking-change-for-an-api.htm)

---

## price data management

### [SUMP] brain price data should be static per slug

the vision assumes price data is static and declared per brain. however, claim [7] shows prices change frequently.

### [KHUE] how often should static price data be updated?

### [OPIN] attribute names should reflect format

> [33] "Leverage attribute names that reflect their format or intent. For example, consumers might not know whether providerCost would be in dollars or cents, so specify with providerCostInDollars or providerCostInCents." — [freecodecamp](https://www.freecodecamp.org/news/how-to-handle-breaking-changes/)

---

## summary of claims

| type | count | description |
|------|-------|-------------|
| FACT | 21 | indisputable, verifiable truths |
| SUMP | 4 | assumptions made by the vision or industry |
| KHUE | 4 | open questions to consider |
| OPIN | 5 | subjective opinions to weigh |

---

## sources

1. mobisoft — llm api price guide
2. mobisoft — claude sonnet 4.5 price ratio
3. anthropic sdk documentation — usage object
4. openai api reference — usage object
5. anthropic prompt cache documentation — cache token rates
6. openai prompt cache documentation — cached input discount
7. aimultiple — llm price january 2026 changes
8. helicone — hidden costs in llm operations
9. helicone — core metrics to track
10. helicone — cost review cadence
11. traceloop — cost attribution via metadata
12. portkey — budget limits auto-prevention
13. openrouter — guardrail budget enforcement
14. openrouter — lower limit wins
15. emergentmind — llm routers address single-model limitations
16. emergentmind — cascade router pattern
17. emergentmind — route cost reduction
18. futureagi — cheaper model adequacy
19. getmaxim — enterprise multi-model strategies
20. llm-stats — mmlu saturation
21. llm-stats — mmlu score progression
22. llm-stats — swe-bench verified gold standard
23. llm-stats — claude opus 4.5 swe-bench score
24. llm-stats — no single score
25. aimultiple — claude context window
26. aimultiple — gpt-4o context window
27. chroma research — context window degradation
28. berkeley — bfcl leaderboard
29. whatllm — tool use success rates
30. microsoft — return type break
31. nordicapis — additive changes non-break
32. bennadel — semantic break
33. freecodecamp — attribute name format
